{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "##  PREPROCESSING  ##\n",
    "#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_lab.data_processing import create_tf_dataset, load_and_preprocess_data\n",
    "\n",
    "# Preprocess data paths\n",
    "image_dir = r'D:\\01_Arnaud\\Etudes\\04_CNAM\\RCP209\\Projet\\CodeV3\\deep-lab3\\data\\VOCdevkit\\VOC2012\\JPEGImages'\n",
    "mask_dir = r'D:\\01_Arnaud\\Etudes\\04_CNAM\\RCP209\\Projet\\CodeV3\\deep-lab3\\data\\VOCdevkit\\VOC2012\\SegmentationClass'\n",
    "\n",
    "train_images, val_images, train_masks, val_masks = load_and_preprocess_data(image_dir, mask_dir)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = create_tf_dataset(train_images, train_masks)\n",
    "val_dataset = create_tf_dataset(val_images, val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "## TRAINING & FINE-TUNING ##\n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_lab3.learning_rate import PolyDecay\n",
    "from deep_lab3.metrics import MeanIoU\n",
    "from deep_lab3.model_gpt import DeepLabV3Plus\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "\n",
    "initial_lr = 0.001\n",
    "epochs = 50\n",
    "poly_decay = PolyDecay(initial_learning_rate=initial_lr, max_epochs=epochs)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=poly_decay, weight_decay=0.0005)\n",
    "\n",
    "model = DeepLabV3Plus(dropout_rate=0.5)\n",
    "# input_shape = (224, 224, 3)\n",
    "# input = tf.keras.Input(shape=input_shape)\n",
    "# model = Model(inputs=input, outputs=model.call(input))\n",
    "# for layer in model.layers:\n",
    "#     if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "#         layer.trainable = False\n",
    "\n",
    "# for layer in model.aspp.layers + model.decoder.layers:\n",
    "#     layer.trainable = True\n",
    "\n",
    "# model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy', MeanIoU(num_classes=NUM_CLASSES)])\n",
    "\n",
    "# history = model.fit(train_dataset, validation_data=val_dataset, epochs=epochs)\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy', MeanIoU()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Add.call().\n\n\u001b[1mDimensions must be equal, but are 14 and 4 for '{{node DeepLabV3Plus_1/backbone_1/resnet101_1/conv4_block1_add_1/Add}} = AddV2[T=DT_FLOAT](DeepLabV3Plus_1/backbone_1/resnet101_1/conv4_block1_0_bn_1/batchnorm/add_1, DeepLabV3Plus_1/backbone_1/resnet101_1/conv4_block1_3_bn_1/batchnorm/add_1)' with input shapes: [?,14,14,1024], [?,4,4,1024].\u001b[0m\n\nArguments received by Add.call():\n  • inputs=['tf.Tensor(shape=(None, 14, 14, 1024), dtype=float32)', 'tf.Tensor(shape=(None, 4, 4, 1024), dtype=float32)']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\01_Arnaud\\Etudes\\04_CNAM\\RCP209\\Projet\\CodeV3\\deep-lab3\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\01_Arnaud\\Etudes\\04_CNAM\\RCP209\\Projet\\CodeV3\\deep-lab3\\src\\deep_lab3\\model_gpt.py:26\u001b[0m, in \u001b[0;36mDeepLabV3Plus.call\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 26\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maspp(x, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[0;32m     28\u001b[0m     low_level_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mresnet_model\u001b[38;5;241m.\u001b[39mget_layer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconv2_block3_out\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39moutput  \u001b[38;5;66;03m# Example of low-level feature extraction\u001b[39;00m\n",
      "File \u001b[1;32mD:\\01_Arnaud\\Etudes\\04_CNAM\\RCP209\\Projet\\CodeV3\\deep-lab3\\src\\deep_lab3\\layers.py:131\u001b[0m, in \u001b[0;36mBackbone.call\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m    129\u001b[0m output_stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodify_resnet_layers(output_stride\u001b[38;5;241m=\u001b[39moutput_stride)\n\u001b[1;32m--> 131\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcascaded_blocks(x, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Add.call().\n\n\u001b[1mDimensions must be equal, but are 14 and 4 for '{{node DeepLabV3Plus_1/backbone_1/resnet101_1/conv4_block1_add_1/Add}} = AddV2[T=DT_FLOAT](DeepLabV3Plus_1/backbone_1/resnet101_1/conv4_block1_0_bn_1/batchnorm/add_1, DeepLabV3Plus_1/backbone_1/resnet101_1/conv4_block1_3_bn_1/batchnorm/add_1)' with input shapes: [?,14,14,1024], [?,4,4,1024].\u001b[0m\n\nArguments received by Add.call():\n  • inputs=['tf.Tensor(shape=(None, 14, 14, 1024), dtype=float32)', 'tf.Tensor(shape=(None, 4, 4, 1024), dtype=float32)']"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping # type: ignore\n",
    "\n",
    "# Définition de l'arrêt anticipé (early stopping)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save the model\n",
    "model.save('results/my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "##  EVALUATION  ##\n",
    "##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "## INFERENCE EXEMPLE  ## \n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display result for one image\n",
    "val_images, val_masks = next(iter(val_dataset))\n",
    "image = val_images[0]\n",
    "prediction = model.predict(tf.expand_dims(image, axis=0))\n",
    "predicted_mask = tf.argmax(prediction, axis=-1)\n",
    "predicted_mask = tf.squeeze(predicted_mask)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Input Image\")\n",
    "plt.imshow(image)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"True Mask\")\n",
    "plt.imshow(tf.squeeze(val_masks[0]), cmap='gray')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Predicted Mask\")\n",
    "plt.imshow(predicted_mask, cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate on the offcial testin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
